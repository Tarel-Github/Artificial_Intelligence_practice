{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNW7/eTYb9pzsH7QLiT8qWo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 정형 데이터와 비정형 데이터\n","지금까지 우리가 다루어왔던 자료들이 전부 정형데이터였다.  \n","정형데이터란  길이, 높이, 무게 등을 데이터로 사용하는 것이다.  \n","뿐만 아니라 프로그래머가 다루는 대부분의 데이터가 정형 데이터이다.  \n","비정형 데이터란 액셀이나 데이터베이스로 표현하기 어려운 것들이다.  \n","예를 들어, 글, 사진, 음악 등이 있다.  \n","  \n","정형 데이터를 가장 잘 다루는 알고리즘이 바로 앙상블 학습이다.  \n","이것은 결정트리를 기반으로 만들어져있다.  \n","이번에 배울 것이 바로 이 앙상블 알고리즘이다.  \n","\n","# 랜덤 포레스트\n","앙상블 학습의 대표 주자 중 하나로 가장 널리 사용된다.  \n","결정트리를 랜덤하게 만들어 결정 트리 숲을 형성한다.  \n","그리고 각 결정 트리의 예측을 통해 최종 예측을 구축한다.  \n","  \n","예를 들어, 1000개의 샘플 중 100개의 샘플을 뽑을 때, 하나를 먼저  \n","확인한 다음 다시 넣고 다시 샘플을 뽑는다면 중복이 발생할 수 있다.  \n","이렇게 만들어진 샘플을 부트스트랩 샘플이라고 한다.  \n","기본적으로, 부트스트랩 샘플은 훈련세트의 크기와 같게 한다.  \n","즉 1000개의 샘플이 있으면 1000번 샘플을 확인한다. (중복허용)  \n","  \n","이러한 랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에  \n","훈련세트에 과대적합되는 것을 막아주고 검증세트와 테스트 세트에서 안정적인  \n","성능을 얻을 수 있다.  \n"],"metadata":{"id":"PS9-JHTYbFNF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"-Y2ND894a7zj","executionInfo":{"status":"ok","timestamp":1680925665195,"user_tz":-540,"elapsed":2938,"user":{"displayName":"손민성","userId":"15408184320550549324"}}},"outputs":[],"source":["# 자료를 준비, 훈련세트와 테스트 세트로 분리\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","wine = pd.read_csv('https://bit.ly/wine_csv_data')\n","data = wine[['alcohol','sugar', 'pH']].to_numpy()\n","target = wine['class'].to_numpy()\n","train_input, test_input, train_target, test_target = train_test_split(data, target, test_size = 0.2, random_state = 42)"]},{"cell_type":"code","source":["# cross_validate() 함수를 사용한 교차검증 수행\n","from sklearn.model_selection import cross_validate\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_jobs=-1, random_state = 42)\n","scores = cross_validate(rf, train_input, train_target, return_train_score= True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score'])) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Svqq6Juben7","executionInfo":{"status":"ok","timestamp":1680925669409,"user_tz":-540,"elapsed":4218,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"2c69bfe2-ee57-49c4-a4ac-d03e3401da49"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9973541965122431 0.8905151032797809\n"]}]},{"cell_type":"code","source":["# 앞의 랜덤 포레스트 모델을 훈련시키고 특성 중요도를 출력\n","rf.fit(train_input, train_target)\n","print(rf.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9opXzmErxcq","executionInfo":{"status":"ok","timestamp":1680925670920,"user_tz":-540,"elapsed":1515,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"e2a89ff3-72d4-4243-a594-5a5d123da263"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.23167441 0.50039841 0.26792718]\n"]}]},{"cell_type":"code","source":["# 자체적으로 모델을 평가하는 점수\n","rf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state=42)\n","rf.fit(train_input, train_target)\n","print(rf.oob_score_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kDe0pOH1sSwW","executionInfo":{"status":"ok","timestamp":1680925672254,"user_tz":-540,"elapsed":1338,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"08f15b78-19c6-44f6-b7a9-74177a158bb4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8934000384837406\n"]}]},{"cell_type":"markdown","source":["# 엑스트라 트리\n","엑스트라 트리는 랜덤 포레스트와 매우 비슷하기 동작한다.  \n","기본적으로 100개의 결정트리를 훈련한다.  \n","차이점은 부트스트랩 샘플을 사용하지 않는다는 점이다.  \n","즉, 결정 트리를 만들 때, 전체 훈련 세트를 사용한다.  \n","대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라  \n","무작위분할을 한다.  \n","하나의 결정트리에서 특성을 무작위로 분할한다면 성능은 낮아지지만  \n","많은 트리를 앙상블하기 때문에 과대적합을 막고 검증세트 점수를  \n","높이는 효과가 있다.  \n","사이킷 런에서 제공하는 엑스트라 트리는 ExtraTreeClassifier다.  \n","이 모델의 교차검증 점수를 확인해보자"],"metadata":{"id":"_8PRZKCjvXbM"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesClassifier\n","et = ExtraTreesClassifier(n_jobs=-1, random_state =42)\n","scores = cross_validate(et, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtqFgCV8vErW","executionInfo":{"status":"ok","timestamp":1680925675543,"user_tz":-540,"elapsed":3292,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"fe88547a-da19-44cb-b106-a6743c29d1fe"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9974503966084433 0.8887848893166506\n"]}]},{"cell_type":"markdown","source":["여기서는 예제의 특성이 많지 않아 랜덤포레스트와 엑스트라 트리의 차이가 적다.  \n","보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤포레스트보다 더 많은  \n","결정트리를 훈련해야 한다.  \n","하지만 랜덤 노드 분할 덕분에 빠른 계산 속도가 장점이다.  "],"metadata":{"id":"eqJ21MHh3ZqM"}},{"cell_type":"code","source":["# 엑스트라 트리 훈련 후 특성 중요도 출력\n","et.fit(train_input, train_target)\n","print(et.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpWAjiKR3rlD","executionInfo":{"status":"ok","timestamp":1680925676183,"user_tz":-540,"elapsed":644,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"b5f5682b-4e7d-4286-b6ce-b020c9c0ec19"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.20183568 0.52242907 0.27573525]\n"]}]},{"cell_type":"markdown","source":["# 그레이디언트 부스팅\n","깊이가 얕은 결정트리를 사용해서 이전트리의 오차를 보완하는 방식으로 하는 앙상블  \n","이른바 경사하강법을 사용해서 트리를 앙상블에 추가하는 방식이다.  \n","그레이언트 부스팅은 결정트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동한다.  \n","때문에 깊이가 얕은 트리를 사용한다."],"metadata":{"id":"ynuLUYvl34-a"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","gb = GradientBoostingClassifier(random_state = 42)\n","scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6ydttJbApON","executionInfo":{"status":"ok","timestamp":1680925679461,"user_tz":-540,"elapsed":3281,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"6b21fac4-c2d4-41d4-8d92-aa3a8e65a9e2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8881086892152563 0.8720430147331015\n"]}]},{"cell_type":"code","source":["# 결정트리 개수를 500개로 늘림\n","gb = GradientBoostingClassifier(n_estimators= 500, learning_rate=0.2, random_state =42)\n","scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdZJLN0YBKY-","executionInfo":{"status":"ok","timestamp":1680925691051,"user_tz":-540,"elapsed":11592,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"0b64a6c1-75b4-4087-99b2-07e3023e14be"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9464595437171814 0.8780082549788999\n"]}]},{"cell_type":"code","source":["gb.fit(train_input, train_target)\n","print(gb.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOCcL5HXCkY8","executionInfo":{"status":"ok","timestamp":1680925692926,"user_tz":-540,"elapsed":1878,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"9ac5c16b-711a-4465-9157-da51ee6f1bcb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.15872278 0.68010884 0.16116839]\n"]}]},{"cell_type":"markdown","source":["# 히스토그램 기반 그레이디언트 부스팅\n","히스토그램 기반 그레이디언트 부스팅은 정형데이터를 다루는 머신러닝 알고리즘 중  \n","가장 인기가 높은 알고리즘이다.  \n","먼저 입력틍성을 256개의 구간으로 나눈 다음 하나를 뗴어 놓고 누락된 값을 위해 사용한다.  \n","따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다.  \n"],"metadata":{"id":"c0sr417FCyLE"}},{"cell_type":"code","source":["from sklearn.experimental import enable_hist_gradient_boosting\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","hgb = HistGradientBoostingClassifier(random_state = 42)\n","scores = cross_validate(hgb, train_input, train_target, return_train_score =True)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3C3tlyIBCw9p","executionInfo":{"status":"ok","timestamp":1680925694929,"user_tz":-540,"elapsed":2010,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"5b58e0dd-101f-4470-98a6-0347db215d58"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["0.9321723946453317 0.8801241948619236\n"]}]},{"cell_type":"code","source":["from sklearn.inspection import permutation_importance\n","hgb.fit(train_input, train_target)\n","result = permutation_importance(hgb, train_input, train_target, n_repeats = 10, random_state = 42, n_jobs = -1)\n","print(result.importances_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6pDkllHDvb1","executionInfo":{"status":"ok","timestamp":1680925697639,"user_tz":-540,"elapsed":2715,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"d8fb59d2-3b27-44a5-d4e1-d9fcb67ce240"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.08876275 0.23438522 0.08027708]\n"]}]},{"cell_type":"code","source":["result = permutation_importance(hgb, test_input, test_target, n_repeats = 10, random_state = 42, n_jobs = -1)\n","print(result.importances_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oaepgz8gEKoy","executionInfo":{"status":"ok","timestamp":1680925698620,"user_tz":-540,"elapsed":985,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"4b0a7943-07cf-4c19-be37-0a6f3461a21f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.05969231 0.20238462 0.049     ]\n"]}]},{"cell_type":"code","source":["hgb.score(test_input, test_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I19v6dw0EaLa","executionInfo":{"status":"ok","timestamp":1680925698620,"user_tz":-540,"elapsed":6,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"e1a968c3-57ed-4227-ebd7-2605290de578"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8723076923076923"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","xgb = XGBClassifier(tree_method = 'hist', random_state = 42)\n","scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0ChELpm-jOF","executionInfo":{"status":"ok","timestamp":1680925856329,"user_tz":-540,"elapsed":1528,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"4f003eb0-647c-46fe-ee44-1d67477a8048"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9555033709953124 0.8799326275264677\n"]}]},{"cell_type":"markdown","source":["# LightGBM²\n","널리 사용하는 또 다른 히스토그램 기반 그레이디언트 부스팅 라이브러리는  \n","마이크로소프트에서 개발한 LightGBM이다.  \n","이것은 빠르고 최신기술이 많이 적용되어 있어 인기가 점점 높아지고 있다."],"metadata":{"id":"_v0-OGaO_u4a"}},{"cell_type":"code","source":["from lightgbm import LGBMClassifier\n","lgb = LGBMClassifier(random_state=42)\n","scores = cross_validate(lgb, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2Z2isDO_FVQ","executionInfo":{"status":"ok","timestamp":1680925966452,"user_tz":-540,"elapsed":776,"user":{"displayName":"손민성","userId":"15408184320550549324"}},"outputId":"d86cafbb-bf73-4e05-af00-6d303e0d52be"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0.935828414851749 0.8801251203079884\n"]}]},{"cell_type":"markdown","source":["# 정리\n","## ■ 앙상블 학습\n","앙상블 학습은 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는   머신러닝 알고리즘이다.  \n","  \n","## ■ 랜덤 포레스트\n","대표적인 결정트리 기반의 앙상블 학습 방법, 부트스트랩 샘플을 사용하고  \n","랜덤하게 일부 특성을 선택해 트리를 만든다.  \n","  \n","## ■ 엑스트라 트리\n","랜덤 포레스트와 비슷하게 결정 트리를 사용해서 앙상블 모델을 만든다.  \n","그러나 부트스트랩 샘플을 사용하지 않는다.  \n","대신 랜덤 노드 분할을 통해 과대적합을 감소시킨다.  \n","  \n","## ■ 그레이디언트 부스팅\n","랜덤 포레스트나 엑스트라 트리와 달리 결정트리를 연속저으로 추가하여 손실 함수를  \n","최소화하는 앙상블 방법이다.  \n","때문에 훈련속도가 조금 느리지만 더 좋은 성능을 기대할 수 있다.  \n","그레이디언트 부스팅의 속도를 개선한 것이 히스토그램 기반 그레이디언트  부스팅이며 안정적인 결과와 높은 성능으로 인기가 높다.  \n","\n","\n"],"metadata":{"id":"jGNytObJAaTr"}}]}