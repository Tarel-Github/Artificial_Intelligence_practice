{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO27UF06ww+312Dbr8qL0pc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RA-lZuripDeU"},"outputs":[],"source":["# 기본적인 임포트\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","source":["# 과대적합과 과소적합\n","\n","과대적합이란 모델이 훈련세트에만 좋은 성능을 내고 검증세트에는 나쁜 성능을 내는 경우다.  \n","일반적으로, 훈련세트 정확도가 99%, 검증세트 정확도가 80%라면 과대적합을 의심할 수 있다.\n","\n","</br>\n","\n","과소적합은 그냥 성능자체가 구린경우다.  \n","\n","</br>\n","\n","즉, 우리는 성능도 우수하면서 검증세트 정확도와 훈련세트 정확도의 차이가 거의 없는  \n","모델을 만드는 것이 훌륭한 모델을 만드는 것이라 할 수 있다."],"metadata":{"id":"d9ToeZckpjvQ"}},{"cell_type":"markdown","source":["## 에포크 손실함수을 통한 과대적합과 과소적합 분석하기\n"],"metadata":{"id":"eXj9ibaipjpQ"}},{"cell_type":"markdown","source":[],"metadata":{"id":"aW-GqPlRpji3"}},{"cell_type":"code","source":[],"metadata":{"id":"jA4eu7gppjKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 지난 시간에 만든 코드\n","class SingleLayer:\n","# 지난시간에 만든 SingleLayer 클래스 수정\n","  def __init__(self, learning_rate=0.1, l1=0, l2=0):\n","      self.w = None\n","      self.b = None\n","      self.losses = []\n","      self.val_losses = []\n","      self.w_history = []\n","      self.lr = learning_rate\n","      self.l1 = l1\n","      self.l2 = l2\n","\n","  def forpass(self, x):\n","    z = np.sum(x * self.w) + self.b   \n","    return z\n","\n","  def backprop(self, x, err):\n","    w_grad = x * err                 \n","    b_grad = 1 * err                 \n","    return w_grad, b_grad\n","\n","  def fit(self, x, y, epochs=100):\n","    self.w = np.ones(x.shape[1])\n","    self.b = 0\n","    self.w_history.append(self.w.copy())\n","    np.random.seed(42)\n","    for i in range(epochs):\n","      loss = 0\n","      indexes = np.random.permutation(np.arange(len(x)))\n","      for i in indexes:\n","        z = self.forpass(x[i])\n","        a = self.activation(z)\n","        err = -(y[i] - a)\n","        w_grad, b_grad = self.backprop(x[i], err)\n","        self.w -= self.lr * w_grad\n","        self.b -= b_grad\n","        # 가중치를 기록합니다.\n","        self.w_history.append(self.w.copy())\n","        # 안전한 로그 계산을 위해 클리핑한 후 손실을 누적한다.  \n","        a = np.clip(a, 1e-10, 1-1e-10)\n","        loss += -(y[i] * np.log(a) + (1-y[i]) * np.log(1-a))\n","      # 에포크 마다 평균 손실을 저장합니다.\n","      self.losses.append(loss/len(y))\n","\n","  # 시그모이드 함수\n","  def activation(self, z):\n","    z = np.clip(z, -100, None)\n","    a = 1 / (1 + np.exp(-z))\n","    return a\n","\n","  def predict(self, x):\n","    z = [self.forpass(x_i) for x_i in x]    # 정방향 계산\n","    return np.array(z) > 0                  # 계단함수 적용\n","\n","  def score(self, x, y):            # 정확도를 계산해주는 메서드\n","    return np.mean(self.predict(x) == y)\n","\n","  def update_val_loss(self, x_val, y_val):\n","    if x_val is None:\n","      return\n","    val_loss = 0\n","    for i in range(len(x_val)):\n","      z = self.forpass(x_val[i])        # 정방향 계산\n","      a = self.activation(z)            # 활성화 함수 적용\n","      a = np.clip(a, 1e-10, 1-1e-10)    \n","      val_loss += -(y_val[i] * np.log(a) + (1 - y_val[i]) * np.log(1-a))\n","    self.val_losses.append(val_loss/len(y_val) + self.reg_loss())"],"metadata":{"id":"wF0PGcRtpOa9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"6KNWJBE3pOMv"}}]}