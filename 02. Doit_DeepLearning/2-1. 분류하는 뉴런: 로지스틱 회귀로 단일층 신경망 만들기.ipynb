{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOKhKfalUPdqQtdK2qI1I8Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 일반적인 신경망\n","\n","일반적으로 신경망은 입력층, 은닉층, 출력층으로 구성된다.  \n","앞서 배운 로지스틱 회귀는 은닉층이 없는 신경망이라고 할 수 있다.  \n","  \n","은닉층과 출력층에는 활성화 함수가 붙어있다.  \n","  \n","  </br>\n","\n","## 단일층 신경망 구현\n","\n","앞서 만든 LogisticNeruon 클래스를 가져온 다음 이름을  \n","singleLayer로 바꾼뒤 코드를 수정해주자  \n","  \n","기존 코드에 경사하강법 등의 기능을 추가할 것이다."],"metadata":{"id":"qxX-aCTqnm7J"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"tppaLkU3nN68","executionInfo":{"status":"ok","timestamp":1682698779839,"user_tz":-540,"elapsed":307,"user":{"displayName":"손민성","userId":"15408184320550549324"}}},"outputs":[],"source":["# 지난번에 만든 코드\n","class LogisticNeuron:\n","  def __init__(self):                 \n","    self.w = None\n","    self.b = None\n","    self.losses = []                  # 손실함수의 결과값을 저장할 리스트 추가\n","  \n","  def forpass(self, x):\n","    z = np.sum(x * self.w) + self.b   \n","    return z\n","\n","  def backprop(self, x, err):\n","    w_grad = x * err                 \n","    b_grad = 1 * err                 \n","    return w_grad, b_grad\n","\n","  def fit(self, x, y, epochs = 100):\n","    self.w = np.ones(x.shape[1])\n","    self.b = 0\n","    for i in range(epochs):\n","      for x_i, y_i in zip(x, y):\n","        z = self.forpass(x_i)\n","        a = self.activation(z)\n","        err = -(y_i -a)\n","        w_grad, b_grad = self.backprop(x_i, err)\n","        self.w -= w_grad\n","        self.b -= b_grad\n","    \n","    for i in index:\n","      z = self.forpass(x[i])              # 정방향 계산\n","      a = self.activation(z)              # 활성화 함수 적용\n","      err = -(y[i] - a)                   # 오차 계산\n","      w_grad, b_grad = self.backprop(x[i], err)   # 역방향 계산\n","      self.w -= w_grad                                # 가중치 업데이트\n","      self.b -= b_grad                                # 절편 업데이트\n","      # 안전한 로그 계산을 위해 클리핑 한 후 손실을 누적\n","      a = np.clip(a, 1e-10, 1-1e-10)\n","      loss += -(y[i] * np.log(a) + (1-y[i]) * np.log(1-a))  # 에포크마다 평균 손실을 저장\n","    \n","    self.losses.append(loss/len(y))\n","\n","  # 시그모이드 함수\n","  def activation(self, z):\n","    a = 1 / (1 + np.exp(-z))\n","    return a\n","\n","  def predict(self, x):\n","    z = [self.forpass(x_i) for x_i in x]\n","    a = self.activation(np.array(z))\n","    return a > 0.5"]},{"cell_type":"markdown","source":["# 여러 가지 경사 하강법\n","  \n","지금까지 경사하강법은 샘플 1개에 대한 그레이디언트(손실함수의 변화율)을 계산했다.  \n","이를 확률적 경사 하강법이라고 한다.  \n","  \n","전체 훈련세트를 사용해 한 번에 그레이디언트를 계산하는 방식인 배치 경사 하강법과  \n","배치 크기를 작게하여(훈련 세트를 여러번 나눠서) 처리하는 미니 배치 경사 하강법이 있다.  \n","</br>\n","\n","\n"],"metadata":{"id":"lhI2p7y2quRo"}}]}